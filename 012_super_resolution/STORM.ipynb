{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.245712Z",
     "iopub.status.busy": "2024-06-20T13:02:44.244974Z",
     "iopub.status.idle": "2024-06-20T13:02:44.248981Z",
     "shell.execute_reply": "2024-06-20T13:02:44.248411Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#  Copyright 2024 Technical University of Denmark\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "#  Authored by: Rafael Flock (DTU)\n",
    "#  Edited by: Chao Zhang (DTU)\n",
    "#  Reviewed by: Jakob Sauer JÃ¸rgensen (DTU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UQ for stochastic optical reconstruction microscopy\n",
    "\n",
    "### Rafael Flock (raff@dtu.dk)\n",
    "\n",
    "**Notice** You need CUQIpy-PyTorch to run this notebook.\n",
    "\n",
    "In this case study, we perform uncertainty quantification for stochastic optical reconstruction microscopy (STORM). STORM is a super-resolution microscopy method based on single-molecule stochastic switching, where the goal is to detect molecule positions in live cell imaging. The image is obtained by a microscope detecting the photon count of the (fluorescence) photoactivated molecules. The original problem is solved in [1] by a deterministic method. In [2], uncertainty quantification is carried out in the Bayesian context. This notebook reimplements the methodology of [2] using CUQIpy.\n",
    "\n",
    "Laplace priors offer an appealing choice in Bayesian inference due to their ability to promote sparsity and handle heavy-tailed distributions. However, their application in high-dimensional inverse problems becomes increasingly challenging.  As the dimensionality of the parameter space grows, the performance of traditional sampling methods employed for Bayesian inference deteriorates significantly.\n",
    "\n",
    "To address this obstacle, this work proposes a novel approach that combines the method of Certified Coordinate Selection (CCS) with a gradient-based sampling method. CCS allows us to efficiently identify the components in the unknown that contribute most to the update from the prior to the posterior, effectively reducing the dimensionality of the problem considered during inference. This enables the gradient-based sampling method to perform efficiently even in high-dimensional settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:37.453101Z",
     "iopub.status.busy": "2024-06-20T13:02:37.450074Z",
     "iopub.status.idle": "2024-06-20T13:02:44.211819Z",
     "shell.execute_reply": "2024-06-20T13:02:44.198043Z"
    }
   },
   "outputs": [],
   "source": [
    "import cuqi\n",
    "import torch\n",
    "from cuqi.distribution import JointDistribution\n",
    "from cuqipy_pytorch.distribution import Gaussian, Laplace\n",
    "from cuqipy_pytorch.sampler import NUTS\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import lognorm, norm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cuqi version: \", cuqi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.280022Z",
     "iopub.status.busy": "2024-06-20T13:02:44.278785Z",
     "iopub.status.idle": "2024-06-20T13:02:44.298051Z",
     "shell.execute_reply": "2024-06-20T13:02:44.297527Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem description\n",
    "\n",
    "We start by collecting some parameters in a dictionary. These parameters will be used in creating our groundtruth image and describing our noise model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.302555Z",
     "iopub.status.busy": "2024-06-20T13:02:44.301360Z",
     "iopub.status.idle": "2024-06-20T13:02:44.306696Z",
     "shell.execute_reply": "2024-06-20T13:02:44.306194Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "par = \\\n",
    "{'N': 50, # number of molecules\n",
    " 's_data': 32, # side length of data image\n",
    " 's_mol': 24, # side length of area in data image where molecules are located\n",
    " 'R': 4, # super-resolution factor\n",
    " 'mode': 3000, # mode of lognormal distribution of molecule photon count\n",
    " 'ph_back': 70, # photon background count\n",
    " 'noise_std': 30, # standard deviation of noise\n",
    " 'pad_width': 1, # for boundary conditions of blurring kernel of instrument\n",
    " 'ext_mode': 'periodic', # boundary condition of blurring kernel\n",
    " 'lognorm_std': 0.417, # standard deviation of lognormal distribution of photon count\n",
    " 'ind_mol_seed': 0, # seed for generation of true molecule positions\n",
    " 'N_pho_seed': 1, # seed for generation of true photon count of molecules\n",
    " 'noise_seed': 2, # seed for noise\n",
    " 'delta': 1.275, # rate parameter of Laplace prior\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define two functions that help us to switch between image and vector view of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.311100Z",
     "iopub.status.busy": "2024-06-20T13:02:44.309899Z",
     "iopub.status.idle": "2024-06-20T13:02:44.314701Z",
     "shell.execute_reply": "2024-06-20T13:02:44.314194Z"
    }
   },
   "outputs": [],
   "source": [
    "# ravel/unravel image/vector in column-wise fashion\n",
    "rav = lambda x: np.ravel(x, order='F')\n",
    "unrav = lambda x, s: np.reshape(x, newshape=(s,s), order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth image\n",
    "\n",
    "Let's break down the process to generate the ground-truth image step by step:\n",
    "\n",
    "- generate true molecule positions: the exact positions of 50 molecules are first generated by drawing samples from a uniform distribution.\n",
    "\n",
    "- generate true molecule counts: The exact molecule count/intensity is simulated from a lognormal distribution of given mean and standard deviation.\n",
    "\n",
    "- place molecules in super-resolution image: the generated molecule counts are then placed in an image of higher resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.319155Z",
     "iopub.status.busy": "2024-06-20T13:02:44.318003Z",
     "iopub.status.idle": "2024-06-20T13:02:44.327711Z",
     "shell.execute_reply": "2024-06-20T13:02:44.327191Z"
    }
   },
   "outputs": [],
   "source": [
    "# true molecule positions\n",
    "x_im_truth = np.zeros((par['s_mol']*par['R'],\n",
    "                       par['s_mol']*par['R']))\n",
    "np.random.seed(par['ind_mol_seed'])\n",
    "ind_mol_crop = np.random.choice(np.arange(x_im_truth.size),\n",
    "                                size=par['N'],\n",
    "                                replace=False) # indices of molecule positions\n",
    "\n",
    "# true photon counts\n",
    "# (mean of lognormal distribution based on [1])\n",
    "lognorm_mean = np.log(par['mode']) + par['lognorm_std']**2 \n",
    "np.random.seed(par['N_pho_seed'])\n",
    "N_pho = lognorm.rvs(loc=0, s=par['lognorm_std'],\n",
    "                    scale=np.exp(lognorm_mean),\n",
    "                    size=par['N']) # photons count of molecules\n",
    "\n",
    "# place photons in super-resolution image\n",
    "x_im_truth[np.unravel_index(ind_mol_crop,\n",
    "                            shape=(x_im_truth.shape[0], x_im_truth.shape[1]),\n",
    "                            order='F')] = N_pho\n",
    "x_im_truth = np.pad(x_im_truth,\n",
    "                    (par['s_data']-par['s_mol'])//2*par['R'],\n",
    "                    mode='constant',\n",
    "                    constant_values=0) # no molecules in this area\n",
    "# (indices of molecules in column-stacked vector)\n",
    "ind_mol = np.nonzero(rav( (x_im_truth > np.zeros((par['s_data']*par['R'],\n",
    "                                                  par['s_data']*par['R'])))\n",
    "                                                  ) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward operator and data\n",
    "We now import the full forward operator for STORM from external file. The forward operator is represented by a matrix $A_{\\text{mat}}$ that maps the true image of molecules to the data image $y$, so $y_{\\text{truth}}=A_{\\text{mat}}x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.332274Z",
     "iopub.status.busy": "2024-06-20T13:02:44.331095Z",
     "iopub.status.idle": "2024-06-20T13:02:44.388411Z",
     "shell.execute_reply": "2024-06-20T13:02:44.387878Z"
    }
   },
   "outputs": [],
   "source": [
    "A_mat = scipy.sparse.load_npz(\"A_mat.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data is then generated by adding Gaussian noise of a specific standard deviation to the data image, so $y = y_{\\text{truth}} + \\text{noise}$. Here we also show the signal-to-noise-ratio (SNR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.399740Z",
     "iopub.status.busy": "2024-06-20T13:02:44.398635Z",
     "iopub.status.idle": "2024-06-20T13:02:44.433088Z",
     "shell.execute_reply": "2024-06-20T13:02:44.432209Z"
    }
   },
   "outputs": [],
   "source": [
    "# true data\n",
    "y_truth = A_mat @ rav(x_im_truth)\n",
    "\n",
    "# noise\n",
    "np.random.seed(par['noise_seed'])\n",
    "N_pho_noise = norm.rvs(scale=par['noise_std'], size=par['s_data']**2)\n",
    "y = y_truth + N_pho_noise\n",
    "print(f\"SNR={np.linalg.norm(y)/np.sqrt(y.size)/par['noise_std']}\") # signal-to-noise-ratio\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better view of the problem, we plot the groundtruth and the data image here. The goal of this notebook is then to estimate to the groundtruth from the data image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:44.439093Z",
     "iopub.status.busy": "2024-06-20T13:02:44.437970Z",
     "iopub.status.idle": "2024-06-20T13:02:45.038218Z",
     "shell.execute_reply": "2024-06-20T13:02:45.037707Z"
    }
   },
   "outputs": [],
   "source": [
    "## plot truth and data\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "\n",
    "# truth\n",
    "im = ax[0].imshow(x_im_truth, cmap='gray')\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "ax[0].set_title('Truth')\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "# data\n",
    "im = ax[1].imshow(unrav(y, par['s_data']), cmap='gray')\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "ax[1].set_title('Noisy data')\n",
    "fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Smoothed Laplace prior\n",
    "\n",
    "The Laplace prior is a type of prior distribution that is frequently used in Bayesian statistics. It is characterized by a peak at zero and exponential decay for both positive and negative values, making it a good choice for promoting sparsity: parameters with a Laplace prior tend to be either significantly non-zero or exactly zero.\n",
    "\n",
    "One of difficulties of working with the Laplace prior is that its probability density function (PDF) is not differentiable at zero and this non-differentiability hinders the use of gradient-based MCMC samplers. Here we use the \"smoothed\" Laplace distribution which approximates the Laplace distribution but is equipped with a continuous gradient method. More details on the smoothed Laplace distribution can be found in a separate [notebook](https://github.com/CUQI-DTU/CUQIpy-User-Showcase/blob/main/007_smoothed_Laplace/smoothed_laplace.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.042394Z",
     "iopub.status.busy": "2024-06-20T13:02:45.041309Z",
     "iopub.status.idle": "2024-06-20T13:02:45.047720Z",
     "shell.execute_reply": "2024-06-20T13:02:45.047243Z"
    }
   },
   "outputs": [],
   "source": [
    "class LaplaceSmoothed(cuqi.distribution.Distribution):\n",
    "    def __init__(self, location, scale, beta, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.location = location\n",
    "        self.scale = scale\n",
    "        self.beta = beta\n",
    "  \n",
    "    def logpdf(self, x):\n",
    "        if isinstance(x, (float,int)):\n",
    "            x = torch.tensor([x])\n",
    "        return torch.sum(torch.log(0.5/self.scale) - torch.sqrt((x-self.location)**2+self.beta)/self.scale)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        x.requires_grad = True\n",
    "        x.grad = None\n",
    "        Q = self.logpdf(x)     # Forward pass\n",
    "        Q.backward()           # Backward pass\n",
    "        return x.grad\n",
    "\n",
    "    def _sample(self,N=1,rng=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimension-reduced likelihood\n",
    "\n",
    "The dimension of the inverse problem is of size $16384=128\\times128$, which is challenging for even gradient-based samplers. In [2], we propose a method, the Certified Coordinate Selection, to reduce the dimension of the problem. In brief, we select the components in $x$ that contribute the most to the posterior relatively to the prior. To this end, we employ the coordinate splitting\n",
    "$$x = (x_{\\mathcal{I}}, x_{\\mathcal{I}^c}),$$\n",
    "where $x_{\\mathcal{I}}$ are the selected coordinates. Then, we replace the likelihood by a ridge approximation $x_{\\mathcal{I}} \\mapsto \\tilde{\\pi}(x_{\\mathcal{I}}|y)$ such that the posterior reads $\\pi(x|y) \\propto \\tilde{\\pi}(x_{\\mathcal{I}}|y) \\pi_0(x)$. With a proper choice of the set $\\mathcal{I}$, an optimal posterior approximation (with respect to the Hellinger distance) can be constructed on $x_{\\mathcal{I}}$. The ridge approximation of the likelihood function reads $\\tilde{\\pi}(x_{\\mathcal{I}}|y) \\propto \\exp( -\\frac{\\lambda}{2} \\| y - A (x_{\\mathcal{I}}, x_{\\mathcal{I}^c=0})\\|^2)$, where $\\lambda$ is the noise precision of the iid Gaussian noise. Note that $x_{\\mathcal{I}}$ is not computed in this notebook, but loaded from a file. We refer the reader to [2] for further information.\n",
    "\n",
    "The selected coordinates $x_{\\mathcal{I}}$ are plotted in the following plot as black pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.051774Z",
     "iopub.status.busy": "2024-06-20T13:02:45.050712Z",
     "iopub.status.idle": "2024-06-20T13:02:45.237550Z",
     "shell.execute_reply": "2024-06-20T13:02:45.237023Z"
    }
   },
   "outputs": [],
   "source": [
    "# truth\n",
    "selected_set = np.load('I.npy')\n",
    "x_selected_set = np.zeros(x_im_truth.shape)\n",
    "fig = plt.figure()\n",
    "\n",
    "temp = rav(x_selected_set)\n",
    "temp[selected_set] = 1\n",
    "x_selected_set = unrav(temp, 128)\n",
    "plt.imshow(x_selected_set, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Posterior sampling with CUQIpy\n",
    "With the above description of the smoothed Laplace prior and reduced likelihood, it is time to have a look at how to do the posterior sampling with CUQIpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.241882Z",
     "iopub.status.busy": "2024-06-20T13:02:45.240721Z",
     "iopub.status.idle": "2024-06-20T13:02:45.252275Z",
     "shell.execute_reply": "2024-06-20T13:02:45.251734Z"
    }
   },
   "outputs": [],
   "source": [
    "# prior\n",
    "d = len(selected_set) # dimension of the problem after reduction\n",
    "beta = 1e-5 # smoothing parameter\n",
    "X = LaplaceSmoothed(location = torch.zeros(d), scale=1/par['delta']*torch.ones(d), beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced forward solver can be defined by selecting specific columns of the imported full matrix. We also utilize a PyTorch tensor for compatibility with PyTorch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.256664Z",
     "iopub.status.busy": "2024-06-20T13:02:45.255502Z",
     "iopub.status.idle": "2024-06-20T13:02:45.271015Z",
     "shell.execute_reply": "2024-06-20T13:02:45.270484Z"
    }
   },
   "outputs": [],
   "source": [
    "A_reduced_coo = A_mat[:,selected_set].tocoo()\n",
    "values = A_reduced_coo.data\n",
    "indices = np.vstack((A_reduced_coo.row, A_reduced_coo.col))\n",
    "\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = A_reduced_coo.shape\n",
    "\n",
    "A_reduced_torch = torch.sparse.FloatTensor(i, v, torch.Size(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.275104Z",
     "iopub.status.busy": "2024-06-20T13:02:45.274007Z",
     "iopub.status.idle": "2024-06-20T13:02:45.279276Z",
     "shell.execute_reply": "2024-06-20T13:02:45.278784Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = Gaussian(lambda X: torch.matmul(A_reduced_torch, X),\n",
    "             cov= par['noise_std']**2*torch.ones(A_reduced_torch.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define our joint distribution and condition it on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.283542Z",
     "iopub.status.busy": "2024-06-20T13:02:45.282391Z",
     "iopub.status.idle": "2024-06-20T13:02:45.289756Z",
     "shell.execute_reply": "2024-06-20T13:02:45.289265Z"
    }
   },
   "outputs": [],
   "source": [
    "X_Y = JointDistribution(X, Y)\n",
    "print(X_Y)\n",
    "\n",
    "p = X_Y(Y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the posterior object `p` defined, we are ready to draw samples from `p`. As we said earlier, the prior is now equipped with gradient method, we can use gradient-based samplers to draw samplers from the posterior. Here we use the NUTS sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.293856Z",
     "iopub.status.busy": "2024-06-20T13:02:45.292731Z",
     "iopub.status.idle": "2024-06-20T13:02:45.302344Z",
     "shell.execute_reply": "2024-06-20T13:02:45.301822Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "nuts_sampler = NUTS(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T13:02:45.306739Z",
     "iopub.status.busy": "2024-06-20T13:02:45.305551Z",
     "iopub.status.idle": "2024-06-21T03:38:18.622880Z",
     "shell.execute_reply": "2024-06-21T03:38:18.622213Z"
    }
   },
   "outputs": [],
   "source": [
    "nuts_samples = nuts_sampler.sample(5000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T03:38:18.628260Z",
     "iopub.status.busy": "2024-06-21T03:38:18.626937Z",
     "iopub.status.idle": "2024-06-21T03:38:19.859050Z",
     "shell.execute_reply": "2024-06-21T03:38:19.858406Z"
    }
   },
   "outputs": [],
   "source": [
    "nuts_samples['X'].plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are only performing posterior sampling on the selected set of pixels. To visualize the posterior mean of the whole image, as described in [2], we can insert these sampled values back into a vector of the same size as the original image, leaving the rest of the entries as zero. More specifically, for $x_{\\mathcal{I}}$, the posterior mean is computed from our NUTS samples, while for $x_{\\mathcal{I}^c}$, the posterior mean is the same as the prior mean, i.e., zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T03:38:19.862179Z",
     "iopub.status.busy": "2024-06-21T03:38:19.861906Z",
     "iopub.status.idle": "2024-06-21T03:38:20.156517Z",
     "shell.execute_reply": "2024-06-21T03:38:20.155971Z"
    }
   },
   "outputs": [],
   "source": [
    "x_post = np.zeros(16384)\n",
    "x_post[selected_set] = nuts_samples['X'].mean()\n",
    "plt.imshow(unrav(x_post, 128), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(\"Posterior mean of NUTS samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize a particular sample from the posterior, specifically, the last sample from the NUTS samples. Similar to viewing the posterior mean, we place this sample into a vector of the same size as the original image, while the remaining pixels are drawn from the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-21T03:38:20.161084Z",
     "iopub.status.busy": "2024-06-21T03:38:20.159853Z",
     "iopub.status.idle": "2024-06-21T03:38:20.451383Z",
     "shell.execute_reply": "2024-06-21T03:38:20.450832Z"
    }
   },
   "outputs": [],
   "source": [
    "x_laplace = Laplace(0, 1/par['delta'])\n",
    "x_last_sample = x_laplace.sample(16384).samples.flatten().numpy()\n",
    "x_last_sample[selected_set] = nuts_samples['X'].samples[:,-1]\n",
    "plt.imshow(unrav(x_last_sample, 128), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(\"The last NUTS sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the ground truth, the posterior mean of the MCMC sampling recovers the signal quite well. Most values converge to near zero, indicating areas with little to no signal. However, a few values converge to higher extremes, highlighting regions where the signal is present. This contrast effectively captures the underlying structure of the image, demonstrating the ability of the Laplace prior in promoting sparsity in signals.\n",
    "\n",
    "We note that the efficient sampling for such a problem is made possible with a combined use of several techniques. The smoothed Laplace prior ensures that the solution remains sparse and interpretable, the reduced likelihood keeps the computational demands manageable, and NUTS guarantees robust and efficient exploration of the posterior distribution. Together, these methods enable the precise reconstruction of the signal.\n",
    "\n",
    "While a comparison of applying a reduced likelihood and the original likelihood with NUTS would be insightful, it is not performed in this work due to the computational demands of UQ with the high-dimensional original likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]\n",
    "Zhu, L., Zhang, W., Elnatan, D., Huang, B.: Faster STORM using compressed sensing. Nature Methods 9(7), 721â723 (2012) https://doi.org/10.1038/nmeth.1978\n",
    "\n",
    "[2]\n",
    "Flock, R., Dong, Y., Uribe, F., & Zahm, O.: Certified coordinate selection for high-dimensional Bayesian inversion with Laplace prior. Statistics and Computing. Under Review (2023). https://doi.org/10.21203/rs.3.rs-3471448/v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
